{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face_recognition from image\n",
    "import os\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "def facePredict(img_path, h5_path):\n",
    "    model = load_model(h5_path)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (128, 128))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = img.reshape((1, 1, 128, 128))\n",
    "    img = img.astype('float32')\n",
    "    img = img/255.0\n",
    "    faceProb = model.predict(img)/max(model.predict(img)[0])\n",
    "    faceLabel = np.argmax(model.predict(img))\n",
    "    return faceProb, faceLabel\n",
    "\n",
    "\n",
    "#print(facePredict(\"E:/testtt/n003554/1575082734424.jpg\", \"D:/桌面/FaceFinal.h5\"))\n",
    "#faceProb, faceLabel = facePredict(\"E:/testttt/276.jpg\", \"D:/桌面/FaceFinal.h5\")\n",
    "faceProb, faceLabel = facePredict(\"jj.jpg\",\"D:/桌面/FaceFinal.h5\")\n",
    "print(faceProb)\n",
    "print(faceLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy \n",
    "import librosa\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import python_speech_features as psf\n",
    "\n",
    "\n",
    "\n",
    "#simple processing of the input signal\n",
    "def getWavFeatures(wavPath):\n",
    "    signal,rate = librosa.load(wavPath, sr=16000)\n",
    "    if len(signal) >= 3 * 16000:\n",
    "        signal = signal[0:int(3 * 16000)]\n",
    "    else:\n",
    "        signal = signal.tolist()\n",
    "        for j in range(3 * 16000 - len(signal)):\n",
    "            signal.append(0)\n",
    "        signal = np.array(signal)\n",
    "    feat = psf.logfbank(signal, samplerate=16000, nfilt=64)\n",
    "    feat1 = psf.delta(feat, 1)\n",
    "    feat2 = psf.delta(feat, 2)\n",
    "    feat = feat.T[:, :, np.newaxis]\n",
    "    feat1 = feat1.T[:, :, np.newaxis]\n",
    "    feat2 = feat2.T[:, :, np.newaxis]\n",
    "    fBank = np.concatenate((feat, feat1, feat2), axis=2)\n",
    "    a = []\n",
    "    a.append(fBank)\n",
    "    fBank = np.array(a)\n",
    "    # print(fBank.shape)\n",
    "    return fBank\n",
    "\n",
    "def voicePredict(wav,h5):\n",
    "    feat = getWavFeatures(wav)\n",
    "    model = load_model(h5)\n",
    "    voiceProb = model.predict(feat)\n",
    "    voiceLabel = np.argmax(voiceProb[0])\n",
    "    return voiceProb, voiceLabel\n",
    "    \n",
    "voiceProb, voiceLabel = voicePredict(\"E:data_aishell/wav/train/S0002/BAC009S0002W0130.wav\", \"D:/桌面/voice 9546.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = np.add(faceProb,voiceProb)\n",
    "#combine = faceProb*voiceProb\n",
    "print(combine)\n",
    "print(np.argmax(combine[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gray\n",
    "#put all the images in one list and crop out the faces\n",
    "def greyScaleAndSave(sourcePath,objectPath,*format):\n",
    "    try:\n",
    "        # read the images\n",
    "        resultArray=readAllImg(sourcePath,*format)\n",
    "\n",
    "        #crop out the faces are save into a new path\n",
    "\n",
    "        count = 0\n",
    "        face_cascade = cv2.CascadeClassifier(\"D:\\Anaconda\\envs\\py36\\Lib\\site-packages\\cv2\\data\\haarcascade_frontalface_default.xml\")\n",
    "        #while count < 20:\n",
    "        for i in resultArray:\n",
    "\n",
    "            #if type(resultArray[count]) != str: \n",
    "            if count == 20:\n",
    "                break\n",
    "            elif type(i) != str:\n",
    "\n",
    "                #gray = cv2.cvtColor(resultArray[count], cv2.COLOR_BGR2GRAY)\n",
    "                gray = cv2.cvtColor(i, cv2.COLOR_BGR2GRAY)\n",
    "                faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "                for (x, y, w, h) in faces:\n",
    "                    # number the files use it as the file name\n",
    "                    fileName = str(count)\n",
    "                    #print(fileName)\n",
    "\n",
    "                    f = cv2.resize(gray[y:(y + h), x:(x + w)], (200, 200))\n",
    "                    cv2.imwrite(objectPath+os.sep+'%s.jpg' % fileName, f)\n",
    "                    count += 1\n",
    "\n",
    "\n",
    "    except IOError:\n",
    "        print (\"Error, the file may not be found\")\n",
    "\n",
    "    else:\n",
    "        #print ('Successfully add '+str(count-1)+' Faces to Destination ' + objectPath)\n",
    "        pass\n",
    "if __name__ == '__main__':\n",
    "  L = []\n",
    "  path = 'E:/FR/vggface2_test/testt/'\n",
    "  \n",
    "  try:\n",
    "    os.makedirs(\"E:/testttt/\")\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  for i in sorted(os.listdir(path)): \n",
    "    L.append(i)\n",
    "    path = os.path.join(\"E:/testttt/\" + i)\n",
    "    os.makedirs(path)\n",
    "  for i in L:\n",
    "    greyScaleAndSave(\"E:/FR/vggface2_test/testt/\"+i,'E:/testttt/'+i,'.jpg','.JPG','png','PNG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "def facePredict(img_path, h5_path):\n",
    "    model = load_model(h5_path)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (128, 128))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = img.reshape((1, 1, 128, 128))\n",
    "    img = img.astype('float32')\n",
    "    img = img/255.0\n",
    "    faceProb = model.predict(img)/max(model.predict(img)[0])\n",
    "    faceLabel = np.argmax(model.predict(img))\n",
    "    return faceProb, faceLabel\n",
    "bigArray = np.zeros((200,20,1,200))\n",
    "path = \"faceTestt/\"\n",
    "countt = 0\n",
    "for i in sorted(os.listdir(path)): \n",
    "  count = 0\n",
    "  for j in sorted(os.listdir(path+i)):\n",
    "    subPath = path+i+\"/\"+j\n",
    "    faceProb, faceLabel = facePredict(subPath, \"FaceFinal.h5\")\n",
    "    bigArray[countt][count] = faceProb\n",
    "    count += 1\n",
    "  countt += 1\n",
    "np.save(\"a.npy\",bigArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fp = np.memmap(\"D:/桌面/test.dat\",dtype='float32',mode='w+',shape=(200,20,1,200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fp = np.memmap(\"D:/桌面/test.dat\",mode=\"r+\",shape=(200,20,1,200))\n",
    "np.shape(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp[0][0] = faceProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import shutil\n",
    "os.makedirs(\"E:/VoiceTest/\")\n",
    "L = []\n",
    "path = \"E:/voice/\"\n",
    "for i in sorted(os.listdir(path)): \n",
    "  L.append(i)\n",
    "  newPath = os.path.join(\"E:/VoiceTest/\" + i)\n",
    "  #os.makedirs(newPath)\n",
    "for i in L:\n",
    "  count = 0\n",
    "  os.makedirs(\"E:/VoiceTest/\"+i)\n",
    "  for j in sorted(os.listdir(\"E:/voice/\" + i))[:20]:\n",
    "    shutil.copyfile(\"E:/voice/\"+i+\"/\"+j,\"E:/VoiceTest/\"+i+\"/\"+str(count)+\".wav\") \n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import python_speech_features as psf\n",
    "\n",
    "\n",
    "\n",
    "#simple processing of the input signal\n",
    "def getWavFeatures(wavPath):\n",
    "    signal,rate = librosa.load(wavPath, sr=16000)\n",
    "    if len(signal) >= 3 * 16000:\n",
    "        signal = signal[0:int(3 * 16000)]\n",
    "    else:\n",
    "        signal = signal.tolist()\n",
    "        for j in range(3 * 16000 - len(signal)):\n",
    "            signal.append(0)\n",
    "        signal = np.array(signal)\n",
    "    feat = psf.logfbank(signal, samplerate=16000, nfilt=64)\n",
    "    feat1 = psf.delta(feat, 1)\n",
    "    feat2 = psf.delta(feat, 2)\n",
    "    feat = feat.T[:, :, np.newaxis]\n",
    "    feat1 = feat1.T[:, :, np.newaxis]\n",
    "    feat2 = feat2.T[:, :, np.newaxis]\n",
    "    fBank = np.concatenate((feat, feat1, feat2), axis=2)\n",
    "    a = []\n",
    "    a.append(fBank)\n",
    "    fBank = np.array(a)\n",
    "    # print(fBank.shape)\n",
    "    return fBank\n",
    "\n",
    "def voicePredict(wav,h5):\n",
    "    feat = getWavFeatures(wav)\n",
    "    model = load_model(h5)\n",
    "    voiceProb = model.predict(feat)\n",
    "    voiceLabel = np.argmax(voiceProb[0])\n",
    "    return voiceProb, voiceLabel\n",
    "#voiceProb, voiceLabel = voicePredict(\"testt/S0002/0.wav\",\"/content/drive/My Drive/Colab Notebooks/voice 9546.h5\")\n",
    "#print(voiceProb)\n",
    "\n",
    "#bigArray = np.zeros((200,20,1,200))\n",
    "fp = np.memmap(\"E:/testt.dat\",mode = \"w+\",shape=(200,20,1,200))\n",
    "path = \"E:/VoiceTest/\"\n",
    "countt = 0\n",
    "try:\n",
    "  for i in sorted(os.listdir(path)): \n",
    "    count = 0\n",
    "    for j in sorted(os.listdir(path+i)):\n",
    "      subPath = path+i+\"/\"+j\n",
    "      faceProb, faceLabel = voicePredict(subPath, \"D:/桌面/voice 9546.h5\")\n",
    "      fp[countt][count] = faceProb\n",
    "      count += 1\n",
    "    countt += 1\n",
    "    print(countt)\n",
    "  #np.save(\"b.npy\",bigArray)\n",
    "  del fp\n",
    "except Exception as err:\n",
    "  print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables \n",
    "import numpy as np\n",
    "print(np.shape(faceProb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_path = \"test_data.hdf5\"\n",
    "hdf5_file = tables.open_file(hdf5_path, mode='w')\n",
    "filters = tables.Filters(complevel=5, complib='blosc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = tables.Filters(complevel=5, complib='blosc')\n",
    "earray = hdf5_file.create_earray(\n",
    "\thdf5_file.root,\n",
    "\t'data', # 数据名称，之后需要通过它来访问数据\n",
    "\ttables.Atom.from_dtype(bigArray.dtype), # 设定数据格式（和data1格式相同）\n",
    "\tshape=(0,20,1,200), # 第一维的 0 表示数据可沿行扩展\n",
    "\tfilters=filters,\n",
    "\texpectedrows=800000 # 完整数据大约规模，可以帮助程序提高时空利用效率\n",
    ")\n",
    "# 将 data1 添加进 earray\n",
    "earray.append(bigArray)\n",
    "# 写完之后记得关闭文件\n",
    "hdf5_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_path = \"test_data.hdf5\"\n",
    "hdf5_file = tables.open_file(hdf5_path, mode='r')\n",
    "# 数据名称为 'data'，我们可以通过 .root.data 访问到它\n",
    "hdf5_data = hdf5_file.root.data\n",
    "print(hdf5_data.shape) # (1000, 4096)\n",
    "# 像在内存中一样自由读取数据切片！\n",
    "print(hdf5_data[:10].shape)\n",
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "[[[0. 1. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "def facePredict(img_path, h5_path):\n",
    "    model = load_model(h5_path)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (128, 128))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = img.reshape((1, 1, 128, 128))\n",
    "    img = img.astype('float32')\n",
    "    img = img/255.0\n",
    "    faceProb = model.predict(img)\n",
    "    faceLabel = np.argmax(model.predict(img))\n",
    "    return faceProb, faceLabel\n",
    "bigArray = np.zeros((20,1,200))\n",
    "#fp = np.memmap(\"test.dat\",mode = \"w+\",shape=(200,20,1,200))\n",
    "path = \"E:/testttt/n000009\"\n",
    "count = 0\n",
    "try:\n",
    "    for j in sorted(os.listdir(path)):\n",
    "        subPath = path+\"/\"+j\n",
    "        faceProb, faceLabel = facePredict(subPath, \"D:/桌面/FaceFinal.h5\")\n",
    "        bigArray[count] = faceProb\n",
    "        count += 1\n",
    "        print(count)\n",
    "    #del fp\n",
    "    #np.save(\"a.npy\",bigArray)\n",
    "except Exception as err:\n",
    "  print(err)\n",
    "print(bigArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigArray = np.reshape(bigArray,(1,20,1,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 1, 200)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(bigArray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 1, 200)\n"
     ]
    }
   ],
   "source": [
    "import tables\n",
    "hdf5_path = \"test_data.hdf5\"\n",
    "# 注意这里 mode 应为 'a'\n",
    "hdf5_file = tables.open_file(hdf5_path, mode='a')\n",
    "hdf5_data = hdf5_file.root.data\n",
    "print(hdf5_data.shape) # (1000, 4096)\n",
    "hdf5_data.append(bigArray)\n",
    "hdf5_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tables' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-55766e952b90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhdf5_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhdf5_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhdf5_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (2000, 4096)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mhdf5_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tables' is not defined"
     ]
    }
   ],
   "source": [
    "hdf5_file = tables.open_file(hdf5_path, mode='r')\n",
    "print(hdf5_file.root.data.shape) # (2000, 4096)\n",
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small1\n",
      "small2\n",
      "small3\n",
      "small4\n",
      "small5\n",
      "small6\n",
      "small7\n",
      "small8\n",
      "small9\n",
      "small10\n",
      "small11\n",
      "small12\n",
      "small13\n",
      "small14\n",
      "small15\n",
      "small16\n",
      "small17\n",
      "small18\n",
      "small19\n",
      "small20\n",
      "big1\n",
      "small1\n",
      "small2\n",
      "small3\n",
      "small4\n",
      "small5\n",
      "small6\n",
      "small7\n",
      "small8\n",
      "small9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "def facePredict(img_path, h5_path):\n",
    "    model = load_model(h5_path)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (128, 128))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = img.reshape((1, 1, 128, 128))\n",
    "    img = img.astype('float32')\n",
    "    img = img/255.0\n",
    "    faceProb = model.predict(img)/max(model.predict(img)[0])\n",
    "    faceLabel = np.argmax(model.predict(img))\n",
    "    return faceProb, faceLabel\n",
    "bigArray = np.zeros((20,1,200))\n",
    "#fp = np.memmap(\"test.dat\",mode = \"w+\",shape=(200,20,1,200))\n",
    "\n",
    "\n",
    "import tables\n",
    "hdf5_path = \"test_data.hdf5\"\n",
    "hdf5_file = tables.open_file(hdf5_path, mode='w')\n",
    "filters = tables.Filters(complevel=5, complib='blosc')\n",
    "\n",
    "filters = tables.Filters(complevel=5, complib='blosc')\n",
    "earray = hdf5_file.create_earray(\n",
    "\thdf5_file.root,\n",
    "\t'data', # 数据名称，之后需要通过它来访问数据\n",
    "\ttables.Atom.from_dtype(bigArray.dtype), # 设定数据格式（和data1格式相同）\n",
    "\tshape=(0,20,1,200), # 第一维的 0 表示数据可沿行扩展\n",
    "\tfilters=filters,\n",
    "\texpectedrows=800000 # 完整数据大约规模，可以帮助程序提高时空利用效率\n",
    ")\n",
    "hdf5_file.close()\n",
    "\n",
    "path = \"E:/testttt/\"\n",
    "countt = 0\n",
    "try:\n",
    "    for i in sorted(os.listdir(path)): \n",
    "        count = 0\n",
    "        for j in sorted(os.listdir(path+i)):\n",
    "            subPath = path+i+\"/\"+j\n",
    "            faceProb, faceLabel = facePredict(subPath, \"D:/桌面/FaceFinal.h5\")\n",
    "            bigArray[count] = faceProb\n",
    "            count += 1\n",
    "            print(\"small\"+str(count))\n",
    "        bigArray = np.reshape(bigArray,(1,20,1,200))\n",
    "        hdf5_path = \"test_data.hdf5\"\n",
    "        # 注意这里 mode 应为 'a'\n",
    "        hdf5_file = tables.open_file(hdf5_path, mode='a')\n",
    "        hdf5_data = hdf5_file.root.data\n",
    "        hdf5_data.append(bigArray)\n",
    "        hdf5_file.close()\n",
    "        countt += 1\n",
    "        del bigArray\n",
    "        gc.collect()\n",
    "        bigArray = np.zeros((20,1,200))\n",
    "        print(\"big\"+str(countt))\n",
    "    #np.save(\"a.npy\",bigArray)\n",
    "except Exception as err:\n",
    "  print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "E:/testttt/n000001/0.jpg\n",
      "E:/testttt/n000001/1.jpg\n",
      "E:/testttt/n000001/10.jpg\n",
      "E:/testttt/n000001/11.jpg\n",
      "E:/testttt/n000001/12.jpg\n",
      "E:/testttt/n000001/13.jpg\n",
      "E:/testttt/n000001/14.jpg\n",
      "E:/testttt/n000001/15.jpg\n",
      "E:/testttt/n000001/16.jpg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "def facePredict(img_path, h5_path):\n",
    "    model = load_model(h5_path)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (128, 128))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = img.reshape((1, 1, 128, 128))\n",
    "    img = img.astype('float32')\n",
    "    img = img/255.0\n",
    "    faceProb = model.predict(img)/max(model.predict(img)[0])\n",
    "    faceLabel = np.argmax(model.predict(img))\n",
    "    del img\n",
    "    gc.collect()\n",
    "    return faceProb, faceLabel\n",
    "\n",
    "#fp = np.memmap(\"test.dat\",mode = \"w+\",shape=(200,20,1,200))\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy \n",
    "import librosa\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import python_speech_features as psf\n",
    "\n",
    "\n",
    "\n",
    "#simple processing of the input signal\n",
    "def getWavFeatures(wavPath):\n",
    "    signal,rate = librosa.load(wavPath, sr=16000)\n",
    "    if len(signal) >= 3 * 16000:\n",
    "        signal = signal[0:int(3 * 16000)]\n",
    "    else:\n",
    "        signal = signal.tolist()\n",
    "        for j in range(3 * 16000 - len(signal)):\n",
    "            signal.append(0)\n",
    "        signal = np.array(signal)\n",
    "    feat = psf.logfbank(signal, samplerate=16000, nfilt=64)\n",
    "    feat1 = psf.delta(feat, 1)\n",
    "    feat2 = psf.delta(feat, 2)\n",
    "    feat = feat.T[:, :, np.newaxis]\n",
    "    feat1 = feat1.T[:, :, np.newaxis]\n",
    "    feat2 = feat2.T[:, :, np.newaxis]\n",
    "    fBank = np.concatenate((feat, feat1, feat2), axis=2)\n",
    "    a = []\n",
    "    a.append(fBank)\n",
    "    fBank = np.array(a)\n",
    "    # print(fBank.shape)\n",
    "    return fBank\n",
    "\n",
    "def voicePredict(wav,h5):\n",
    "    feat = getWavFeatures(wav)\n",
    "    model = load_model(h5)\n",
    "    voiceProb = model.predict(feat)\n",
    "    voiceLabel = np.argmax(voiceProb[0])\n",
    "    return voiceProb, voiceLabel\n",
    "\n",
    "L=[]\n",
    "facePath = \"E:/testttt/n000001/\"\n",
    "voicePath = \"E:/VoiceTest/S0002/\"\n",
    "faceModel = \"D:/桌面/FaceFinal.h5\"\n",
    "faceSubPath = sorted(os.listdir(facePath))\n",
    "voiceModel = \"D:/桌面/voice 9546.h5\"\n",
    "voiceSubPath = sorted(os.listdir(voicePath))\n",
    "\n",
    "print(faceSubPath)\n",
    "for i in range(20):\n",
    "    #faceArray = np.zeros(20,1,200)\n",
    "    #voiceArray = np.zeros(20,1,200)\n",
    "    print(facePath + faceSubPath[i])\n",
    "    faceProb, faceLabel = facePredict(facePath + faceSubPath[i],faceModel)\n",
    "    voiceProb, voiceLabrl = voicePredict(voicePath + voiceSubPath[i],voiceModel)\n",
    "    faceProb = faceProb/max(faceProb[0])\n",
    "    voiceProb = voiceProb/max(voiceProb[0])\n",
    "    com = np.add(faceProb,voiceProb)\n",
    "    #print(np.argmax(com))\n",
    "    L.append(np.argmax(com))\n",
    "print(L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i0\n",
      "j0\n",
      "j1\n",
      "j2\n",
      "j3\n",
      "j4\n",
      "j5\n",
      "j6\n",
      "j7\n",
      "j8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "def facePredict(img_path, h5_path):\n",
    "    model = load_model(h5_path)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (128, 128))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = img.reshape((1, 1, 128, 128))\n",
    "    img = img.astype('float32')\n",
    "    img = img/255.0\n",
    "    faceProb = model.predict(img)/max(model.predict(img)[0])\n",
    "    faceLabel = np.argmax(model.predict(img))\n",
    "    del img\n",
    "    gc.collect()\n",
    "    return faceProb, faceLabel\n",
    "\n",
    "#fp = np.memmap(\"test.dat\",mode = \"w+\",shape=(200,20,1,200))\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy \n",
    "import librosa\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import python_speech_features as psf\n",
    "\n",
    "\n",
    "\n",
    "#simple processing of the input signal\n",
    "def getWavFeatures(wavPath):\n",
    "    signal,rate = librosa.load(wavPath, sr=16000)\n",
    "    if len(signal) >= 3 * 16000:\n",
    "        signal = signal[0:int(3 * 16000)]\n",
    "    else:\n",
    "        signal = signal.tolist()\n",
    "        for j in range(3 * 16000 - len(signal)):\n",
    "            signal.append(0)\n",
    "        signal = np.array(signal)\n",
    "    feat = psf.logfbank(signal, samplerate=16000, nfilt=64)\n",
    "    feat1 = psf.delta(feat, 1)\n",
    "    feat2 = psf.delta(feat, 2)\n",
    "    feat = feat.T[:, :, np.newaxis]\n",
    "    feat1 = feat1.T[:, :, np.newaxis]\n",
    "    feat2 = feat2.T[:, :, np.newaxis]\n",
    "    fBank = np.concatenate((feat, feat1, feat2), axis=2)\n",
    "    a = []\n",
    "    a.append(fBank)\n",
    "    fBank = np.array(a)\n",
    "    # print(fBank.shape)\n",
    "    return fBank\n",
    "\n",
    "def voicePredict(wav,h5):\n",
    "    feat = getWavFeatures(wav)\n",
    "    model = load_model(h5)\n",
    "    voiceProb = model.predict(feat)\n",
    "    voiceLabel = np.argmax(voiceProb[0])\n",
    "    return voiceProb, voiceLabel\n",
    "\n",
    "facePath = \"E:/testttt/\"\n",
    "voicePath = \"E:/VoiceTest/\"\n",
    "faceSubPath = sorted(os.listdir(facePath))\n",
    "voiceSubPath = sorted(os.listdir(voicePath))\n",
    "faceModel = \"D:/桌面/FaceFinal.h5\"\n",
    "#faceSubSubPath = sorted(os.listdir(facePath))\n",
    "voiceModel = \"D:/桌面/voice 9546.h5\"\n",
    "#voiceSubSubPath = sorted(os.listdir(voicePath))\n",
    "\n",
    "#print(faceSubPath)\n",
    "for i in range(200):\n",
    "  print(\"i\" + str(i))\n",
    "  faceSubSubPath = sorted(os.listdir(facePath + faceSubPath[i]+\"/\"))\n",
    "  voiceSubSubPath = sorted(os.listdir(voicePath + voiceSubPath[i]+\"/\"))\n",
    "  count  = 0\n",
    "  for j in range(20):\n",
    "      print(\"j\" + str(j))\n",
    "      #faceArray = np.zeros(20,1,200)\n",
    "      #voiceArray = np.zeros(20,1,200)\n",
    "      #print(facePath + faceSubPath[i])\n",
    "      #print(facePath + faceSubPath[i] +\"/\" faceSubSubPath[j] ,faceModel)\n",
    "      faceProb, faceLabel = facePredict(facePath + faceSubPath[i] +\"/\"+ faceSubSubPath[j] ,faceModel)\n",
    "      voiceProb, voiceLabrl = voicePredict(voicePath + voiceSubPath[i] +\"/\"+ voiceSubSubPath[j],voiceModel)\n",
    "      faceProb = faceProb/max(faceProb[0])\n",
    "      voiceProb = voiceProb/max(voiceProb[0])\n",
    "      com = np.add(faceProb,voiceProb)\n",
    "      result = np.argmax(com)\n",
    "      if result == i:\n",
    "        count += 1\n",
    "\n",
    "  f = open(\"record.txt\",\"a\")\n",
    "  f.write(\"count\")\n",
    "  f.close()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
